# Proxy-WASM-compatible shared-memory interface

* Status: proposed
* Deciders: WasmX
* Date: 2022-08-04

## Table of Contents

- [Table of Contents](#table-of-contents)
- [Problem Statement](#problem-statement)
- [Technical Context](#technical-context)
- [Decision Drivers](#decision-drivers)
- [Proposal](#proposal)
  - [C ABI](#c-abi)
    - [Shared memory zone: `ngx_wasm_shm_t`](#shared-memory-zone-ngx_wasm_shm_t)
    - [KV namespaces](#kv-namespaces)
    - [ABI methods](#abi-methods)
  - [Config interface](#config-interface)
  - [Proxy-WASM interface layer](#proxy-wasm-interface-layer)
  - [Internal mechanisms](#internal-mechanisms)
    - [Inter-process event dispatch](#inter-process-event-dispatch)
  - [Known Limitations](#known-limitations)
- [Decision Outcomes](#decision-outcomes)

## Problem Statement

Nginx is a multi-process system where there is one *master* process and multiple *worker* processes. There is a need to share data and communicate between WebAssembly plugin instances in different worker processes, but it is not currently possible to pass data across the process boundary in WasmX.

This document proposes a design of WasmX's C ABI and internal mechanisms for implementing the key-value store and queue API as defined by the proxy-wasm specification, based on the [Nginx shared memory API](http://nginx.org/en/docs/dev/development_guide.html#shared_memory).

[Back to TOC](#table-of-contents)

## Technical Context

In the OpenResty Lua API there is the [shdict](https://github.com/openresty/lua-resty-core/blob/master/lib/resty/core/shdict.lua) module that provides a cross-process communication mechanism to plugins written in Lua. The design in proxy-wasm is similar, in the form of *shared kv stores* and *shared queues*.

On the surface, this is the set of ABI functions defined by proxy-wasm:

```cpp
WasmResult proxy_get_shared_data(const char *key_ptr, size_t key_size,
                                 const char **value_ptr, size_t *value_size,
                                 uint32_t *cas);
WasmResult proxy_set_shared_data(const char *key_ptr, size_t key_size,
                                const char *value_ptr, size_t value_size, uint32_t cas);
WasmResult proxy_register_shared_queue(const char *queue_name_ptr,
                                       size_t queue_name_size, uint32_t *token);
WasmResult proxy_resolve_shared_queue(const char *vm_id, size_t vm_id_size,
                                      const char *queue_name_ptr, size_t queue_name_size,
                                      uint32_t *token);
WasmResult proxy_dequeue_shared_queue(uint32_t token, const char **data_ptr,
                                      size_t *data_size);
WasmResult proxy_enqueue_shared_queue(uint32_t token, const char *data_ptr,
                                      size_t data_size);
```

And the WASM plugin can provide a function to be woke up when a new message arrives in the queue:

```cpp
void proxy_on_queue_ready(uint32_t root_context_id, uint32_t token);
```

The KV store ABI supports single-key transactions with the `cas` parameter. The shared queue ABI exposes a simple pair of enqueue/dequeue functions, without specifying behavior on edge-cases like a full queue.

[Back to TOC](#table-of-contents)

## Decision Drivers

- Aim at `proxy-wasm` full compatibility.
- Have a internal shared memory ABI that is independent from `proxy-wasm`.

[Back to TOC](#table-of-contents)

## Proposal

This section will cover the entire design of the proposed subsystem:

- C ABI
- Config interface
- Proxy-wasm interface layer
- Implementation details

### C ABI

#### Shared memory zone: `ngx_wasm_shm_t`

An `ngx_wasm_shm_t` is stored in the `data` field of an `ngx_shm_zone_t`.

```c
typedef enum {
    NGX_WASM_SHM_TYPE_KV = 0,
    NGX_WASM_SHM_TYPE_QUEUE = 1,
} ngx_wasm_shm_type_e;

struct ngx_wasm_shm_s {
    ngx_wasm_shm_type_e     type;
    ngx_slab_pool_t        *shpool;
    ngx_str_t               name;
    uint32_t               *notification; // array(max_num_workers)
    void                   *data;
};

typedef struct ngx_wasm_shm_s ngx_wasm_shm_t;
```

The list of all shared memory mappings is stored as a field in `ngx_wasm_core_conf_t` and initialized during config load.

```c
struct ngx_wasm_shm_mapping_s {
    ngx_str_t        name;
    ngx_shm_zone_t  *zone;
};

typedef struct ngx_wasm_shm_mapping_s ngx_wasm_shm_mapping_t;

typedef struct {
    // ...
    ngx_array_t  *shms; // element type is `ngx_wasm_shm_mapping_t`
} ngx_wasm_core_conf_t;
```

#### KV namespaces

The `proxy-wasm` ABI does not give access to multiple KV namespaces. From the plugin's point of view, there is one single key-value namespace containing all the KV pairs. The *KV namespace* feature is an optional extension that determines which shared memory zone is used, given a key prefix.

#### ABI methods

```c
// Gets the shared memory zone identified by `name`.
ngx_shm_zone_t *ngx_wasm_shm_get_by_name(
  ngx_wasm_shm_family_t *shm_family,
  ngx_str_t *name);

// KV methods
// Reads a value from the KV namespace. The CAS counter is written to `cas`.
int ngx_wasm_shm_kv_get(ngx_wasm_shm_t *shm,
                        ngx_str_t *key,
                        ngx_str_t **value_out,
                        uint32_t *cas);

// Checks the CAS counter and sets the value of the key.
int ngx_wasm_shm_kv_set(ngx_wasm_shm_t *shm,
                        ngx_str_t *key,
                        ngx_str_t *value,
                        uint32_t cas);

// Queue methods
// Pushes an item into the queue.
int ngx_wasm_shm_queue_push(ngx_wasm_shm_t *shm,
                            ngx_str_t *data);
// Pops an item from the queue.
int ngx_wasm_shm_queue_pop(ngx_wasm_shm_t *shm,
                           ngx_str_t **data_out);

// Listens for item on the queue.        
int ngx_wasm_shm_queue_listen(ngx_wasm_shm_t *shm, ngx_event_t *ev);
```

### Config interface

Shared memory zones are a type of global resource, so they are defined in the `wasm` directive. Binding relations are defined in the same level.

```
wasm {
    module my_filter /path/to/filter.wasm;
    module my_module /path/to/module.wasm;

    #      [name]   [size] [modules...]
    shm_kv *        10mb;                           # attached to all modules without namespacing
    shm_kv my_kv_1  10mb;                           # attached to all modules as the `my_kv_1` namespace
    shm_kv my_kv_2  10mb   my_filter my_module:*;   # attached to `my_filter` as `my_kv_2`, to `my_module` without namespacing
    shm_kv my_kv_3  10mb   my_filter:ns1 my_module; # attached to `my_filter` as `ns1`, to `my_module` as `my_kv_3`

    #         [name]   [size] [modules...]
    shm_queue my_queue 15mb   my_filter my_module:alias1; # attached to `my_filter` as `my_queue`, to `my_module` as `alias1`
}
```

Each module can only be associated with at most one unnamespaced kv.

### Proxy-WASM interface layer

The WasmX C ABI described above mostly directly maps to the Proxy-WASM interface.

### Internal mechanisms

To implement all the proxy-wasm ABI functions as described in the [technical context](#technical-context), the shared memory subsystem needs to serve two purposes: **synchronized data access** for storing and accessing the key-value pairs and enqueued messages, and **event notification** for invoking the `proxy_on_queue_ready` function.

A red-black tree will be used for storing the key-value pairs. Access to the red-black tree is synchronized using a global mutex. This part will be similar to [what OpenResty does](https://github.com/openresty/lua-nginx-module/blob/653d6a36f46b077cb902d7ba40824c299cf9bbf4/src/ngx_http_lua_shdict.c).

The shared queue part is more interesting, as it requires the ability to *notify* another worker asynchronously. A queue is implemented as a circular buffer of bytes. Each message consists of a 4-byte little-endian length header and the message body, in that order. Enqueue and dequeue operations correspond to push and pop on the circular buffer.

On enqueue, *exactly one* listener that previously called `ngx_wasm_shm_queue_listen` on the queue is woke up. The one listener is randomly selected from all listeners on this queue. If this listener is in the same worker process, it is directly woke up using `ngx_post_event`. Otherwise, the *inter-process event dispatch* mechanism is used.

Both intra-process and inter-process wake-ups are asynchronous events. From the perspective of the enqueueing worker, the earliest time a wake-up's effect can be observed is the next tick of the event loop.

#### Inter-process event dispatch

Futex (fast user-space mutex) is an efficient mechanism for cross-process notification, and is supported on all mainstream platforms including Linux ([futex](https://man7.org/linux/man-pages/man2/futex.2.html)), macOS ([ulock](https://opensource.apple.com/source/xnu/xnu-7195.50.7.100.1/bsd/sys/ulock.h.auto.html)) and Windows ([WaitOnAddress](https://docs.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-waitonaddress)). In this proposal, futex will be used as the mechanism for notifying another worker about queue readiness.

Each worker process spawns a *dispatch thread* on start. This dispatch thread is responsible for converting synchronous futex notifications to asynchronous `ngx_post_event` calls.

Pseudo-code for the logic:

```c
void dispatch_thread(uint32_t *notification) {
  uint32_t local_counter = 0;
  while(1) {
    uint32_t new_counter = atomic_load(notification);
    if(new_counter != local_counter) {
      local_counter = new_counter;
      poll_queue_updates(); // calls `ngx_post_event` internally
      continue;
    }
    futex(notification, FUTEX_WAIT, local_counter, NULL, NULL, 0);
  }
}

void poll_queue_updates() {
  int i;
  int cont = 1;

  while(cont) {
    cont = 0;
    for(i = 0; i < num_queues; i++) {
      lock(&queues[i]->lock);
      if(!queue_is_empty(queues[i]) && try_lock(&queues[i]->notification_lock) == LOCK_OK) {
        // the event handler on the main thread is responsible for calling `on_event_completion` 
        ngx_post_event(/* ... */);
        cont = 1;
      }
      unlock(&queues[i]->lock);
    }
  }
}

void on_event_completion(queue_t *q) {
  unlock(&q->notification_lock);
}
```

[Back to TOC](#table-of-contents)

### Known Limitations

- This is a specialized design for proxy-wasm-compatible KV and queues. Data structure additions in the future will require separate host-side design.

[Back to TOC](#table-of-contents)

## Decision Outcomes

TBD.

[Back to TOC](#table-of-contents)
